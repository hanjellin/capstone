# model/predictor.py
from __future__ import annotations

import math
from pathlib import Path
from typing import List, Dict, Any, Optional

import torch

from model.lstm_model import LoadLSTM
from model.dataset import FEATURE_KEYS


class LoadPredictor:
    """
    DFY Assistantìš© ë¶€í•˜ ì˜ˆì¸¡ê¸°.
    ìµœê·¼ seq_lenê°œì˜ snapshot ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„
    - ë‹¤ìŒ ì‹œì ì˜ CPU ì‚¬ìš©ë¥  ì˜ˆì¸¡
    - ìœ„í—˜ë„(0~1), ìƒíƒœ ë¬¸ìì—´ ë°˜í™˜
    """

    def __init__(
        self,
        model_path: str = "internal/model_load_lstm.pth",
        seq_len: int = 30,
        device: Optional[str] = None,
    ) -> None:
        self.seq_len = seq_len

        # ğŸ”½ í”„ë¡œì íŠ¸ ë£¨íŠ¸(new_dfy) ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
        root_dir = Path(__file__).resolve().parents[1]  # .../new_dfy
        mp = Path(model_path)
        if not mp.is_absolute():
            mp = root_dir / mp
        self.model_path = mp

        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = device

        if not self.model_path.exists():
            raise FileNotFoundError(
                f"ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.model_path}\n"
                f"ë¨¼ì € `python -m model.train_lstm` ë¥¼ ì‹¤í–‰í•´ í•™ìŠµì„ ì™„ë£Œí•˜ì„¸ìš”."
            )

        self.model = LoadLSTM(input_dim=len(FEATURE_KEYS))
        state = torch.load(self.model_path, map_location=self.device)
        self.model.load_state_dict(state)
        self.model.to(self.device)
        self.model.eval()

    def _build_sequence(self, history: List[Dict[str, Any]]) -> torch.Tensor:
        """
        history: ìµœê·¼ Nê°œì˜ snapshot (SystemCollector.get_data() í¬ë§·)
        """
        if len(history) < self.seq_len:
            pad_needed = self.seq_len - len(history)
            padding = [history[0]] * pad_needed if history else [{
                k: 0.0 for k in FEATURE_KEYS
            }]
            history = padding + history

        history = history[-self.seq_len :]

        seq = []
        for snap in history:
            vec = []
            for k in FEATURE_KEYS:
                v = snap.get(k, 0.0)
                try:
                    v = float(v) if v is not None else 0.0
                except (TypeError, ValueError):
                    v = 0.0
                vec.append(v)
            seq.append(vec)

        x = torch.tensor(seq, dtype=torch.float32).unsqueeze(0)  # (1, seq_len, dim)
        return x.to(self.device)

    @torch.no_grad()
    def predict_next_cpu(self, history: List[Dict[str, Any]]) -> float:
        x = self._build_sequence(history)
        pred = self.model(x)   # (1, 1)
        return float(pred.item())

    @torch.no_grad()
    def assess_risk(self, history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        history[-1]['cpu'] ì™€ ì˜ˆì¸¡ëœ cpu_next ë¥¼ ë¹„êµí•´ì„œ ìœ„í—˜ë„ ê³„ì‚°.
        """
        if not history:
            return {
                "status": "UNKNOWN",
                "risk_score": 0.0,
                "predicted_cpu": None,
                "current_cpu": None,
                "reason": "no history",
            }

        current_cpu = float(history[-1].get("cpu", 0.0))
        pred_cpu = self.predict_next_cpu(history)

        # ê¸°ì¤€: 75% ì´ìƒì´ë©´ ìœ„í—˜ ì»¤ì§, ê·¸ ì´ìƒì¼ìˆ˜ë¡ risk_score â†‘
        # (pred_cpu - 75) / 7.5 ë¥¼ sigmoidì— ë„£ì–´ì„œ 0~1 ìŠ¤ì¼€ì¼
        z = (pred_cpu - 75.0) / 7.5
        risk_raw = 1 / (1 + math.exp(-z))
        risk_score = max(0.0, min(1.0, risk_raw))

        if risk_score < 0.33:
            status = "NORMAL"
        elif risk_score < 0.66:
            status = "WARN"
        else:
            status = "CRITICAL"

        return {
            "status": status,
            "risk_score": risk_score,
            "predicted_cpu": pred_cpu,
            "current_cpu": current_cpu,
        }
