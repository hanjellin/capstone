# model/ae_detector.py
from __future__ import annotations

import json
from pathlib import Path
from typing import List, Dict, Any, Optional

import torch

from model.ae_model import LoadAutoencoder
from model.dataset import FEATURE_KEYS
from engine import collector

class AEDetector:
    """
    Autoencoder ê¸°ë°˜ ì‹œê³„ì—´ ì´ìƒ íƒì§€ê¸°.

    - ì…ë ¥: ìµœê·¼ history (metrics_buffer.get_feature_history() í¬ë§·)
    - ì¶œë ¥:
        * compute_score() : Reconstruction Error ìŠ¤ì¹¼ë¼
        * classify()      : NORMAL / WARN / CRITICAL ë¶„ë¥˜ + score/thresholds
    """

    def __init__(
        self,
        model_path: str = "internal/model_autoencoder.pth",
        threshold_path: str = "internal/ae_thresholds.json",
        seq_len: int = 30,
        device: Optional[str] = None,
    ) -> None:
        self.seq_len = seq_len
        self.model_path = Path(model_path)
        self.threshold_path = Path(threshold_path)

        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = device

        # --- ëª¨ë¸ ë¡œë“œ ---
        if not self.model_path.exists():
            raise FileNotFoundError(
                f"AE ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.model_path}\n"
                f"ë¨¼ì € 'python -m model.train_ae' ë¡œ Autoencoderë¥¼ í•™ìŠµí•´ ì£¼ì„¸ìš”."
            )

        self.model = LoadAutoencoder(
            seq_len=self.seq_len,
            input_dim=len(FEATURE_KEYS),
        )
        state = torch.load(self.model_path, map_location=self.device)
        self.model.load_state_dict(state)
        self.model.to(self.device)
        self.model.eval()

        # --- ì„ê³„ê°’ ë¡œë“œ ---
        self.thresholds = self._load_thresholds()

    def _load_thresholds(self) -> Dict[str, float]:
        """
        train_ae.pyì—ì„œ ì €ì¥í•œ ae_thresholds.json ë¡œë“œ.
        ì—†ìœ¼ë©´ 0 ê¸°ë°˜ ê¸°ë³¸ê°’ ë°˜í™˜.
        """
        if not self.threshold_path.exists():
            # ê¸°ë³¸ê°’ (ì„ê³„ê°’ ì—†ìœ¼ë©´ ê·¸ëƒ¥ 0ìœ¼ë¡œ ì„¸íŒ…)
            return {
                "mean": 0.0,
                "std": 0.0,
                "warn": 0.0,
                "critical": 0.0,
            }

        with self.threshold_path.open("r", encoding="utf-8") as f:
            data = json.load(f)

        out: Dict[str, float] = {}
        for k in ("mean", "std", "warn", "critical"):
            v = data.get(k, 0.0)
            try:
                out[k] = float(v)
            except (TypeError, ValueError):
                out[k] = 0.0
        return out

    def _build_sequence(self, history: List[Dict[str, Any]]) -> Optional[torch.Tensor]:
        """
        history: ìµœê·¼ Nê°œì˜ snapshot
                 ì˜ˆ) metrics_buffer.get_feature_history() ê²°ê³¼

        ë°˜í™˜: (1, seq_len, dim) í…ì„œ (deviceë¡œ ì˜®ê²¨ì§„ ìƒíƒœ)
        """
        if not history:
            return None

        # ê¸¸ì´ê°€ ëª¨ìë¼ë©´ ì•ìª½ì„ ë³µì œí•´ì„œ íŒ¨ë”©
        if len(history) < self.seq_len:
            pad_needed = self.seq_len - len(history)
            padding = [history[0]] * pad_needed
            history = padding + history

        # ë„ˆë¬´ ê¸¸ë©´ ë’¤ì—ì„œ seq_lenê°œë§Œ ì‚¬ìš©
        history = history[-self.seq_len :]

        seq = []
        for snap in history:
            vec = []
            for k in FEATURE_KEYS:
                v = snap.get(k, 0.0)
                try:
                    v = float(v) if v is not None else 0.0
                except (TypeError, ValueError):
                    v = 0.0
                vec.append(v)
            seq.append(vec)

        x = torch.tensor(seq, dtype=torch.float32).unsqueeze(0)  # (1, seq_len, dim)
        return x.to(self.device)

    @torch.no_grad()
    def compute_score(self, history: List[Dict[str, Any]]) -> Optional[float]:
        """
        Reconstruction Error (MSE)ë¥¼ ìŠ¤ì¹¼ë¼ë¡œ ë°˜í™˜.
        historyê°€ ë¹„ì–´ ìˆìœ¼ë©´ None.
        """
        x = self._build_sequence(history)
        if x is None:
            return None

        # reconstruction_error(reduction="none") â†’ (batch,)
        scores = self.model.reconstruction_error(x, reduction="none")
        return float(scores.item())

    @torch.no_grad()
    def classify(self, history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        history ê¸°ë°˜ Reconstruction Errorë¥¼ ê³„ì‚°í•˜ê³ ,
        warn / critical ì„ê³„ê°’ì— ë”°ë¼ ìƒíƒœë¥¼ ë¶„ë¥˜í•œë‹¤.

        ë°˜í™˜ ì˜ˆ:
        {
            "status": "NORMAL" / "WARN" / "CRITICAL" / "UNKNOWN",
            "score":  0.0032 ë˜ëŠ” None,
            "thresholds": { "mean":..., "std":..., "warn":..., "critical":... },
            "reason": "no history"  # í•„ìš” ì‹œ
        }
        """
        score = self.compute_score(history)

        if score is None:
            return {
                "status": "UNKNOWN",
                "score": None,
                "thresholds": self.thresholds,
                "reason": "no history",
            }

        warn = self.thresholds.get("warn", 0.0)
        critical = self.thresholds.get("critical", warn)

        if critical <= warn:
            # ì„ê³„ê°’ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ì„¤ì •ëœ ê²½ìš° â†’ ì¼ë‹¨ NORMALë¡œ ê°„ì£¼
            status = "NORMAL"
        else:
            if score < warn:
                status = "NORMAL"
            elif score < critical:
                status = "WARN"
            else:
                status = "CRITICAL"

        return {
            "status": status,
            "score": score,
            "thresholds": self.thresholds,
        }
        
    def assess_current_state(self) -> Dict[str, Any]:
        """
            í˜„ì¬ collectorì˜ ì‹¤ì‹œê°„ metricsë¥¼ ì½ì–´ì™€
            reconstruction errorì™€ ìƒíƒœë¥¼ ë°˜í™˜í•œë‹¤.
        ì¶”ê°€ë¡œ, ì–´ë–¤ í•­ëª©ì´ í‰ì†Œì™€ ê°€ì¥ ë‹¤ë¥´ê²Œ íŠ€ì—ˆëŠ”ì§€ë„ í•¨ê»˜ ëŒë ¤ì¤€ë‹¤.
        """
        metrics = collector.get_current_metrics()
        x = self._metrics_to_vector(metrics)

        # ğŸ”» ìƒˆ í—¬í¼ë¡œ ì „ì²´ score + ìƒìœ„ í¸ì°¨ í”¼ì²˜ ê³„ì‚°
        score, top_devs = self._analyze_deviation(x, metrics)

        # ìƒíƒœ íŒì •
        if score >= self.critical_threshold:
            status = "CRITICAL"
        elif score >= self.warn_threshold:
            status = "WARN"
        else:
            status = "NORMAL"

        return {
            "status": status,
            "score": score,
            "warn_threshold": self.warn_threshold,
            "critical_threshold": self.critical_threshold,
            "error_mean": self.error_mean,
            "error_std": self.error_std,
            "num_samples": self.num_samples,
            "metrics": metrics,
            # ğŸ”» ìƒˆ í•„ë“œ: ì–´ë–¤ í”¼ì²˜ê°€ ì–¼ë§ˆë‚˜ íŠ€ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´
            "top_deviations": top_devs,
        }

        # ğŸ”» ìƒˆë¡œ ì¶”ê°€: í”¼ì²˜ë³„ í¸ì°¨(í‰ê·  ëŒ€ë¹„)ì™€ ì—ëŸ¬ë¥¼ ê³„ì‚°í•´ì„œ ìƒìœ„ ëª‡ ê°œë§Œ ë½‘ì•„ì£¼ëŠ” í•¨ìˆ˜
    def _analyze_deviation(
        self,
        x: torch.Tensor,
        metrics: Dict[str, Any],
    ):
        """
        x: (feature_dim,)  í˜„ì¬ ì‹œì ì˜ ì›ë³¸ í”¼ì²˜ ë²¡í„°
        metrics: collector.get_current_metrics() ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

        ë°˜í™˜:
            score: ì „ì²´ reconstruction error (float)
            top_devs: ì´ìƒë„ê°€ í° í”¼ì²˜ ìƒìœ„ ëª‡ ê°œ ë¦¬ìŠ¤íŠ¸
        """
        # í•™ìŠµ ë•Œì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì •ê·œí™”
        x_norm = (x - self.feature_mean) / self.feature_std
        x_norm_batch = x_norm.to(self.device).unsqueeze(0)  # (1, F)

        with torch.no_grad():
            recon = self.model(x_norm_batch)
            err_vec = ((recon - x_norm_batch) ** 2)[0]  # (F,)
            score = float(err_vec.mean().item())

        # z-score ëŠ” x_norm ê°’ ìì²´ê°€ ë¨
        z_vec = x_norm  # (F,)
    def assess_current_state(self) -> Dict[str, Any]:
        """
        í˜„ì¬ collectorì˜ ì‹¤ì‹œê°„ metricsë¥¼ ì½ì–´ì™€
        reconstruction errorì™€ ìƒíƒœë¥¼ ë°˜í™˜í•œë‹¤.
        ì¶”ê°€ë¡œ, ì–´ë–¤ í•­ëª©ì´ í‰ì†Œì™€ ê°€ì¥ ë‹¤ë¥´ê²Œ íŠ€ì—ˆëŠ”ì§€ë„ í•¨ê»˜ ëŒë ¤ì¤€ë‹¤.
        """
        metrics = collector.get_current_metrics()
        x = self._metrics_to_vector(metrics)

        # ğŸ”» ìƒˆ í—¬í¼ë¡œ ì „ì²´ score + ìƒìœ„ í¸ì°¨ í”¼ì²˜ ê³„ì‚°
        score, top_devs = self._analyze_deviation(x, metrics)

        # ìƒíƒœ íŒì •
        if score >= self.critical_threshold:
            status = "CRITICAL"
        elif score >= self.warn_threshold:
            status = "WARN"
        else:
            status = "NORMAL"

        return {
            "status": status,
            "score": score,
            "warn_threshold": self.warn_threshold,
            "critical_threshold": self.critical_threshold,
            "error_mean": self.error_mean,
            "error_std": self.error_std,
            "num_samples": self.num_samples,
            "metrics": metrics,
            # ğŸ”» ìƒˆ í•„ë“œ: ì–´ë–¤ í”¼ì²˜ê°€ ì–¼ë§ˆë‚˜ íŠ€ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´
            "top_deviations": top_devs,
        }

        # í”¼ì²˜ í‚¤ â†’ collector ë©”íŠ¸ë¦­ í‚¤ ë§¤í•‘ (metricsì—ì„œ ì‹¤ì œ ê°’ êº¼ë‚¼ ë•Œ ì‚¬ìš©)
        key_to_metric = {
            "cpu": "cpu_usage",
            "ram": "ram_usage",
            "gpu": "gpu_usage",
            "gpu_temp": "gpu_temp",
            "disk_read": "disk_read",
            "disk_write": "disk_write",
            "net_upload": "net_upload",
            "net_download": "net_download",
        }

        # ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ í•œê¸€ ë¼ë²¨
        nice_labels = {
            "cpu": "CPU ì‚¬ìš©ë¥ ",
            "ram": "RAM ì‚¬ìš©ë¥ ",
            "gpu": "GPU ì‚¬ìš©ë¥ ",
            "gpu_temp": "GPU ì˜¨ë„",
            "disk_read": "ë””ìŠ¤í¬ ì½ê¸° ì†ë„",
            "disk_write": "ë””ìŠ¤í¬ ì“°ê¸° ì†ë„",
            "net_upload": "ì—…ë¡œë“œ ì†ë„",
            "net_download": "ë‹¤ìš´ë¡œë“œ ì†ë„",
        }

        deviations = []
        for idx, key in enumerate(self.feature_keys):
            z = float(z_vec[idx].item())
            err = float(err_vec[idx].item())

            # í˜„ì¬ ì‹¤ì œ ê°’ (ì˜ˆ: CPU ì‚¬ìš©ë¥  %)ë„ ê°™ì´ ë„£ì–´ë‘ë©´ ë‚˜ì¤‘ì— ì“¸ ìˆ˜ ìˆìŒ
            mkey = key_to_metric.get(key, key)
            raw_val = metrics.get(mkey, 0.0)
            try:
                raw_val = float(raw_val)
            except (TypeError, ValueError):
                raw_val = 0.0

            # ë°©í–¥: ë†’ì€ ìª½ìœ¼ë¡œ íŠ / ë‚®ì€ ìª½ìœ¼ë¡œ íŠ / ì• ë§¤
            if z >= 0.5:
                direction = "high"
            elif z <= -0.5:
                direction = "low"
            else:
                direction = "neutral"

            deviations.append(
                {
                    "key": key,
                    "label": nice_labels.get(key, key),
                    "z": z,
                    "error": err,
                    "direction": direction,
                    "value": raw_val,
                }
            )

        # ì ˆëŒ“ê°’ z-scoreê°€ í° ìˆœìœ¼ë¡œ ì •ë ¬
        deviations.sort(key=lambda d: abs(d["z"]), reverse=True)

        # ë„ˆë¬´ ì• ë§¤í•œ ê±´ ë²„ë¦¬ê³ (|z|>=1.0 ì´ìƒë§Œ) ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
        top_devs = [d for d in deviations if abs(d["z"]) >= 1.0][:3]

        return score, top_devs
